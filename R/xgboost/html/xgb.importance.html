<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Importance of features in a model.</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for xgb.importance {xgboost}"><tr><td>xgb.importance {xgboost}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Importance of features in a model.</h2>

<h3>Description</h3>

<p>Creates a <code>data.table</code> of feature importances in a model.
</p>


<h3>Usage</h3>

<pre>
xgb.importance(feature_names = NULL, model = NULL, trees = NULL,
  data = NULL, label = NULL, target = NULL)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>feature_names</code></td>
<td>
<p>character vector of feature names. If the model already
contains feature names, those would be used when <code>feature_names=NULL</code> (default value).
Non-null <code>feature_names</code> could be provided to override those in the model.</p>
</td></tr>
<tr valign="top"><td><code>model</code></td>
<td>
<p>object of class <code>xgb.Booster</code>.</p>
</td></tr>
<tr valign="top"><td><code>trees</code></td>
<td>
<p>(only for the gbtree booster) an integer vector of tree indices that should be included
into the importance calculation. If set to <code>NULL</code>, all trees of the model are parsed.
It could be useful, e.g., in multiclass classification to get feature importances 
for each class separately. IMPORTANT: the tree index in xgboost models
is zero-based (e.g., use <code>trees = 0:4</code> for first 5 trees).</p>
</td></tr>
<tr valign="top"><td><code>data</code></td>
<td>
<p>deprecated.</p>
</td></tr>
<tr valign="top"><td><code>label</code></td>
<td>
<p>deprecated.</p>
</td></tr>
<tr valign="top"><td><code>target</code></td>
<td>
<p>deprecated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function works for both linear and tree models.
</p>
<p>For linear models, the importance is the absolute magnitude of linear coefficients. 
For that reason, in order to obtain a meaningful ranking by importance for a linear model, 
the features need to be on the same scale (which you also would want to do when using either 
L1 or L2 regularization).
</p>


<h3>Value</h3>

<p>For a tree model, a <code>data.table</code> with the following columns:
</p>

<ul>
<li> <p><code>Features</code> names of the features used in the model;
</p>
</li>
<li> <p><code>Gain</code> represents fractional contribution of each feature to the model based on
the total gain of this feature's splits. Higher percentage means a more important 
predictive feature.
</p>
</li>
<li> <p><code>Cover</code> metric of the number of observation related to this feature;
</p>
</li>
<li> <p><code>Frequency</code> percentage representing the relative number of times
a feature have been used in trees.
</p>
</li></ul>

<p>A linear model's importance <code>data.table</code> has the following columns:
</p>

<ul>
<li> <p><code>Features</code> names of the features used in the model;
</p>
</li>
<li> <p><code>Weight</code> the linear coefficient of this feature;
</p>
</li>
<li> <p><code>Class</code> (only for multiclass models) class label.
</p>
</li></ul>

<p>If <code>feature_names</code> is not provided and <code>model</code> doesn't have <code>feature_names</code>, 
index of the features will be used instead. Because the index is extracted from the model dump
(based on C++ code), it starts at 0 (as in C/C++ or Python) instead of 1 (usual in R).
</p>


<h3>Examples</h3>

<pre>

# binomial classification using gbtree:
data(agaricus.train, package='xgboost')
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 2, 
               eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
xgb.importance(model = bst)

# binomial classification using gblinear:
bst &lt;- xgboost(data = agaricus.train$data, label = agaricus.train$label, booster = "gblinear", 
               eta = 0.3, nthread = 1, nrounds = 20, objective = "binary:logistic")
xgb.importance(model = bst)

# multiclass classification using gbtree:
nclass &lt;- 3
nrounds &lt;- 10
mbst &lt;- xgboost(data = as.matrix(iris[, -5]), label = as.numeric(iris$Species) - 1,
               max_depth = 3, eta = 0.2, nthread = 2, nrounds = nrounds,
               objective = "multi:softprob", num_class = nclass)
# all classes clumped together:
xgb.importance(model = mbst)
# inspect importances separately for each class:
xgb.importance(model = mbst, trees = seq(from=0, by=nclass, length.out=nrounds))
xgb.importance(model = mbst, trees = seq(from=1, by=nclass, length.out=nrounds))
xgb.importance(model = mbst, trees = seq(from=2, by=nclass, length.out=nrounds))

# multiclass classification using gblinear:
mbst &lt;- xgboost(data = scale(as.matrix(iris[, -5])), label = as.numeric(iris$Species) - 1,
               booster = "gblinear", eta = 0.2, nthread = 1, nrounds = 15,
               objective = "multi:softprob", num_class = nclass)
xgb.importance(model = mbst)

</pre>

<hr /><div style="text-align: center;">[Package <em>xgboost</em> version 1.0.0.1 <a href="00Index.html">Index</a>]</div>
</body></html>
